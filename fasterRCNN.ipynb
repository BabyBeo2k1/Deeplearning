{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4281320-f5a7-4e79-983b-b5c8c5e089d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import xml.etree.ElementTree as ET\\nimport os\\nimport xmltodict\\nimport torch\\nfrom torch.utils.data import Dataset\\nimport transformers as T\\nimport shutil\\nimport random\\nfrom PIL import Image\\nimport numpy as np'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import xmltodict\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers as T\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7ff9e54-d58a-4abc-a7d6-8fadfac6eee2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef xml_handling(root=\\'./archive\\', target_root=\\'data\\'):\\n    root=\\'./archive\\'\\n    target_root=\\'data\\'\\n    root_label=root+\\'/annotations\\'\\n    root_img=root+\\'/images/\\'\\n    \\n    xml_files=os.listdir(root_label)    \\n    i=0\\n    for file in xml_files:\\n        if file[0]==\\'.\\':\\n            continue\\n        file=root_label+\"/\"+file\\n        train=False\\n        if random.random()<0.8:\\n            train=True\\n        tree = ET.parse(file)\\n        root = tree.getroot()\\n        base_name=os.path.basename(file)\\n        \\n        base_name,ext=os.path.splitext(base_name)\\n        result = ET.tostring(root,encoding=\\'utf-8\\').decode(\\'utf-8\\')\\n        dict_data=xmltodict.parse(result)\\n        data=dict_data[\\'annotation\\'][\\'object\\']\\n        if len(data)==2:\\n            continue\\n        src_img=root_img+base_name+\\'.png\\'\\n        img_path,label_path=\"\",\"\"\\n        if data[\\'name\\']==\\'cat\\':\\n            note=\\'Cat\\'+\" \"+str(data[\\'bndbox\\'][\\'xmin\\'])+\" \"+str(data[\\'bndbox\\'][\\'ymin\\'])+\" \"+str(data[\\'bndbox\\'][\\'xmax\\'])+\" \"+str(data[\\'bndbox\\'][\\'ymax\\'])\\n            if train:\\n                label_path=target_root+\\'/train/Cat/Label/\\'+base_name+\\'.txt\\'\\n                img_path=target_root+\\'/train/Cat/\\'+base_name+\\'.png\\'\\n            else:\\n                label_path=target_root+\\'/val/Cat/Label/\\'+base_name+\\'.txt\\'\\n                img_path=target_root+\\'/val/Cat/\\'+base_name+\\'.png\\'\\n        else:\\n            note=\\'Dog\\'+\" \"+str(data[\\'bndbox\\'][\\'xmin\\'])+\" \"+str(data[\\'bndbox\\'][\\'ymin\\'])+\" \"+str(data[\\'bndbox\\'][\\'xmax\\'])+\" \"+str(data[\\'bndbox\\'][\\'ymax\\'])\\n            \\n            if train:\\n                label_path=target_root+\\'/train/Dog/Label/\\'+base_name+\\'.txt\\'\\n                img_path=target_root+\\'/train/Dog/\\'+base_name+\\'.png\\'\\n            else:\\n                \\n                label_path=target_root+\\'/val/Dog/Label/\\'+base_name+\\'.txt\\'\\n                img_path=target_root+\\'/val/Dog/\\'+base_name+\\'.png\\'\\n        shutil.copy2(src_img,img_path)\\n        print(label_path)\\n        with open(label_path,\\'w\\') as f:\\n            f.write(note)\\n        \\n        \\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def xml_handling(root='./archive', target_root='data'):\n",
    "    root='./archive'\n",
    "    target_root='data'\n",
    "    root_label=root+'/annotations'\n",
    "    root_img=root+'/images/'\n",
    "    \n",
    "    xml_files=os.listdir(root_label)    \n",
    "    i=0\n",
    "    for file in xml_files:\n",
    "        if file[0]=='.':\n",
    "            continue\n",
    "        file=root_label+\"/\"+file\n",
    "        train=False\n",
    "        if random.random()<0.8:\n",
    "            train=True\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        base_name=os.path.basename(file)\n",
    "        \n",
    "        base_name,ext=os.path.splitext(base_name)\n",
    "        result = ET.tostring(root,encoding='utf-8').decode('utf-8')\n",
    "        dict_data=xmltodict.parse(result)\n",
    "        data=dict_data['annotation']['object']\n",
    "        if len(data)==2:\n",
    "            continue\n",
    "        src_img=root_img+base_name+'.png'\n",
    "        img_path,label_path=\"\",\"\"\n",
    "        if data['name']=='cat':\n",
    "            note='Cat'+\" \"+str(data['bndbox']['xmin'])+\" \"+str(data['bndbox']['ymin'])+\" \"+str(data['bndbox']['xmax'])+\" \"+str(data['bndbox']['ymax'])\n",
    "            if train:\n",
    "                label_path=target_root+'/train/Cat/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/train/Cat/'+base_name+'.png'\n",
    "            else:\n",
    "                label_path=target_root+'/val/Cat/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/val/Cat/'+base_name+'.png'\n",
    "        else:\n",
    "            note='Dog'+\" \"+str(data['bndbox']['xmin'])+\" \"+str(data['bndbox']['ymin'])+\" \"+str(data['bndbox']['xmax'])+\" \"+str(data['bndbox']['ymax'])\n",
    "            \n",
    "            if train:\n",
    "                label_path=target_root+'/train/Dog/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/train/Dog/'+base_name+'.png'\n",
    "            else:\n",
    "                \n",
    "                label_path=target_root+'/val/Dog/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/val/Dog/'+base_name+'.png'\n",
    "        shutil.copy2(src_img,img_path)\n",
    "        print(label_path)\n",
    "        with open(label_path,'w') as f:\n",
    "            f.write(note)\n",
    "        \n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3746bb02-7307-49c5-a408-879d74849f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"class AnimalData:\n",
    "    def __init__(self,root,transforms):\n",
    "        self.root=root\n",
    "        self.transforms=transforms\n",
    "        classes=sort(list(os.listdir(root)))\n",
    "        images=[]\n",
    "        for c in classes:\n",
    "            if c=='Label':\n",
    "                continue\n",
    "            for img in sort(list(os.listdir(c))):\n",
    "                images.append(root+c+img)\n",
    "        self.images=images\n",
    "        label_dir=[c+'/Label' for c in classes]\n",
    "        labels=[]\n",
    "        for c in classes:\n",
    "            for label in sort(list(os.listdir(label_dir))):\n",
    "                labels.append(root+label_dir+label)\n",
    "        self.labels=labels\n",
    "    def __getitem__(self,idx):\n",
    "        img =Image.open(self.img[idx]).convert(\"RGB\")\n",
    "        with open(self.labels,'r') as f:\n",
    "            content= f.read()\n",
    "            lines=contents.split(' ')\n",
    "            for i in range(1, len(lines)):\n",
    "                lines[i]=int(lines[i])\n",
    "        \n",
    "        \"\"\"\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#from bs4 import BeautifulSoup\n",
    "import torchvision\n",
    "from torchvision import datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import random\n",
    "import os\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "217cf022-932d-4b3d-955c-f29172ec9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=[]\n",
    "test_labels=[]\n",
    "train_imgs=[]\n",
    "test_imgs=[]\n",
    "classes=['Bear', 'Bull', 'Cat', 'Cattle',\n",
    "           'Cheetah', 'Chicken', 'Crocodile',\n",
    "           'Deer', 'Dog', 'Fox', 'Goat', 'Hippopotamus', \n",
    "           'Horse', 'Jaguar', 'Leopard', 'Lion', 'Lynx',\n",
    "           'Monkey', 'Mule', 'Ostrich', 'Owl', \n",
    "           'Pig', 'Raccoon', 'Raven', 'Rhinoceros', \n",
    "           'Sheep', 'Snake', 'Tiger', 'Turkey', 'Turtle', 'Zebra']\n",
    "\"\"\"classes=['Cat','Dog']\"\"\"\n",
    "encode={}\n",
    "for i in range(len(classes)):\n",
    "    encode[classes[i]]=i+1\n",
    "\n",
    "def checkclass(dirname):\n",
    "    \n",
    "    for animal in classes:\n",
    "        if dirname.endswith(animal):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for dirname, dirs, filenames in os.walk('./data/'):\n",
    "    \n",
    "    if 'train' in dirname and checkclass(dirname):\n",
    "        labels= dirname +'/Label/'\n",
    "        \n",
    "        for label in sorted(os.listdir(labels)):\n",
    "            \n",
    "            \n",
    "            if label.endswith('txt'):\n",
    "                train_labels.append(labels+label)\n",
    "        \n",
    "        for image in sorted(filenames):\n",
    "            \n",
    "            \n",
    "            if image.endswith('png') or image.endswith('jpg'):\n",
    "                train_imgs.append(dirname+'/'+image)\n",
    "    if 'test' in dirname and checkclass(dirname):\n",
    "        labels= dirname +'/Label/'\n",
    "        for label in sorted(os.listdir(labels)):\n",
    "            if label.endswith('.txt'):\n",
    "                test_labels.append(labels+label)\n",
    "        for image in sorted(filenames):\n",
    "            if image.endswith('png'):\n",
    "                test_imgs.append(dirname+'/'+image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9806e096-f681-445d-ba87-9197c87ac4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14620\n",
      "14620\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels))\n",
    "print(len(train_imgs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14d3c431-843f-4748-9e75-b0aa2001eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(object):\n",
    "    def __init__(self, transforms,img_dir,label_dir):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = img_dir\n",
    "        self.label=label_dir\n",
    "#         self.labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        file_image = self.imgs[idx]\n",
    "        file_label = self.label[idx]\n",
    "        img = Image.open(file_image).convert(\"RGB\")\n",
    "        target={}\n",
    "        target['image_id']=torch.tensor([idx])\n",
    "        labels=[]\n",
    "        boxes=[]\n",
    "        #Generate Label\n",
    "        with open(file_label,'r')as f:\n",
    "            lines=f.readlines()\n",
    "            for line in lines:\n",
    "                line_list=line.strip().split(' ')\n",
    "                labels.append(encode[line_list[0]])\n",
    "                boxes.append([float(i) for i in line_list[1:]])\n",
    "                print(len(line_list))\n",
    "            \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)        \n",
    "        \n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        \n",
    "        iscrowd = torch.zeros((len(classes),), dtype=torch.int64)\n",
    "\n",
    "        target['boxes']=boxes\n",
    "        target['labels']=labels\n",
    "       \n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3fe233c-0c69-4006-9c81-73c68b4aefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AugmentImageBboxes(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, image, bboxes):\n",
    "        # decide if flip or rotate\n",
    "        if random.random() < 0.5:\n",
    "            # Flip\n",
    "            image = transforms.functional.hflip(image)\n",
    "            h,w=image.shape[-2:]\n",
    "            bboxes[:, [0, 2]] = h - bboxes[:, [2, 0]] \n",
    "        else:\n",
    "            # Rotate by 90 degrees\n",
    "            image = transforms.functional.rotate(image, 90)\n",
    "            h, w = image.shape[-2:]\n",
    "            bboxes[:, [0, 1, 2, 3]] = [w - bboxes[:, 3], bboxes[:, 0], h - bboxes[:, 1], w - bboxes[:, 0]]\n",
    "        return image, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f33b8b64-c012-4106-9715-36cdf9cd8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transforms as T\n",
    "import utils\n",
    "def get_transform(train):\n",
    "    trans = []\n",
    "    trans.append(T.ToTensor())\n",
    "    if train:\n",
    "        trans.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4a635332",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_model_instance_segmentation(num_classes = 32):\n",
    "    # load an instance segmentation model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # replace the classifier with a new one, that has\n",
    "    # num_classes which is user-defined\n",
    "      # 1 class (person) + background\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d1d2250e-98d2-409a-a81a-87fd431349ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/625 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "\n",
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "from torch.utils.data import DataLoader\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "num_epochs =1\n",
    "model=get_model_instance_segmentation()\n",
    "\n",
    "model.to('cuda')\n",
    "from tqdm import tqdm\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "train_data=AnimalDataset(get_transform(train=True),train_imgs[:10000],train_labels[:10000])\n",
    "    \n",
    "data_loader=DataLoader(\n",
    "        train_data, batch_size=16, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "len_dataloader = len(data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in tqdm(data_loader):\n",
    "        i += 1\n",
    "        print('a')\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        \n",
    "        print('b')\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        \n",
    "        print('c')\n",
    "        loss_dict = model([imgs[0]], [annotations[0]])\n",
    "        \n",
    "        print('d')\n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "        \n",
    "        print('e')\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "#         print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "        epoch_loss += losses\n",
    "    print(epoch_loss)\n",
    "def main():\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # our dataset has two classes only - background and person\n",
    "    num_classes = len(classes)+1\n",
    "    # use our dataset and defined transformations\n",
    "    train_data=AnimalDataset(get_transform(train=True),train_imgs[:10000],train_labels[:10000])\n",
    "    test_data=AnimalDataset(get_transform(train=False),test_imgs[:100],test_labels[:100])\n",
    "    print(\"flag1\")\n",
    "    # define training and validation data loaders\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_data, batch_size=16, shuffle=True, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        test_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "        collate_fn=utils.collate_fn)\n",
    "    print(\"flag2\")\n",
    "    # get the model using our helper function\n",
    "    model = get_model_instance_segmentation(num_classes)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "    # and a learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                   step_size=3,\n",
    "                                                   gamma=0.1)\n",
    "\n",
    "    # let's train it for 10 epochs\n",
    "    num_epochs = 10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(\"flag3\")\n",
    "        # train for one epoch, printing every 10 iterations\n",
    "        train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "        # update the learning rate\n",
    "        print('flag4')\n",
    "        lr_scheduler.step()\n",
    "        # evaluate on the test dataset\n",
    "        evaluate(model, test_loader, device=device)\n",
    "\n",
    "    print(\"That's it!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4978b8a3-63c3-49c5-aa83-e3e6fae137d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag1\n",
      "flag2\n",
      "flag3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53220fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f22ee6d-a640-4ec0-97f2-22dcb16a28c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/914 [00:28<48:24,  3.21s/it]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Target 3 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 19\u001b[0m\n\u001b[0;32m     17\u001b[0m imgs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(img\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m imgs)\n\u001b[0;32m     18\u001b[0m annotations \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m annotations]\n\u001b[1;32m---> 19\u001b[0m loss_dict \u001b[39m=\u001b[39m model([imgs[\u001b[39m0\u001b[39;49m]], [annotations[\u001b[39m0\u001b[39;49m]])\n\u001b[0;32m     20\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())        \n\u001b[0;32m     22\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n\u001b[0;32m    104\u001b[0m proposals, proposal_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrpn(images, features, targets)\n\u001b[1;32m--> 105\u001b[0m detections, detector_losses \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mroi_heads(features, proposals, images\u001b[39m.\u001b[39;49mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform\u001b[39m.\u001b[39mpostprocess(detections, images\u001b[39m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[39m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m losses \u001b[39m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:772\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[1;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[0;32m    770\u001b[0m     \u001b[39mif\u001b[39;00m regression_targets \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    771\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mregression_targets cannot be None\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 772\u001b[0m     loss_classifier, loss_box_reg \u001b[39m=\u001b[39m fastrcnn_loss(class_logits, box_regression, labels, regression_targets)\n\u001b[0;32m    773\u001b[0m     losses \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mloss_classifier\u001b[39m\u001b[39m\"\u001b[39m: loss_classifier, \u001b[39m\"\u001b[39m\u001b[39mloss_box_reg\u001b[39m\u001b[39m\"\u001b[39m: loss_box_reg}\n\u001b[0;32m    774\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\detection\\roi_heads.py:31\u001b[0m, in \u001b[0;36mfastrcnn_loss\u001b[1;34m(class_logits, box_regression, labels, regression_targets)\u001b[0m\n\u001b[0;32m     28\u001b[0m labels \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(labels, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m     29\u001b[0m regression_targets \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(regression_targets, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m classification_loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39;49mcross_entropy(class_logits, labels)\n\u001b[0;32m     33\u001b[0m \u001b[39m# get indices that correspond to the regression targets for\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# the corresponding ground truth labels, to be used with\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m# advanced indexing\u001b[39;00m\n\u001b[0;32m     36\u001b[0m sampled_pos_inds_subset \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mwhere(labels \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 3 is out of bounds."
     ]
    }
   ],
   "source": [
    "num_epochs =1\n",
    "model.to(device)\n",
    "from tqdm import tqdm\n",
    "# parameters\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                                momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "len_dataloader = len(data_loader)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    i = 0    \n",
    "    epoch_loss = 0\n",
    "    for imgs, annotations in tqdm(data_loader):\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model([imgs[0]], [annotations[0]])\n",
    "        losses = sum(loss for loss in loss_dict.values())        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        losses.backward()\n",
    "        optimizer.step() \n",
    "#         print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')\n",
    "        epoch_loss += losses\n",
    "    print(epoch_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee1d38f-5761-450e-88a2-7a1dceeb5764",
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, annotations in data_loader:\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a664821-1588-44c0-8667-9af4210f0f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "preds = model(imgs)\n",
    "preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f649a4c-a6e4-422c-a4dd-ae6ad72ca475",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d9a60-6e8d-4049-b8b2-848b7f1f2c4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3a689c3-71c8-474f-bf25-1cdd4ec93417",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torchvision' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m torchvision\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mdetection\u001b[39m.\u001b[39mfasterrcnn_resnet50_fpn(weights\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mDEFAULT\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m dataset \u001b[39m=\u001b[39m AnimalDataset( get_transform(train\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),train_imgs[:\u001b[39m10000\u001b[39m],train_labels[:\u001b[39m10000\u001b[39m])\n\u001b[0;32m      3\u001b[0m data_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[0;32m      4\u001b[0m  dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m      5\u001b[0m  collate_fn\u001b[39m=\u001b[39mutils\u001b[39m.\u001b[39mcollate_fn)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torchvision' is not defined"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "dataset = AnimalDataset( get_transform(train=True),train_imgs[:10000],train_labels[:10000])\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=2, shuffle=True, num_workers=4,\n",
    " collate_fn=utils.collate_fn)\n",
    "# For Training\n",
    "images,targets = next(iter(data_loader))\n",
    "images = list(image for image in images)\n",
    "targets = [{k: v for k, v in t.items()} for t in targets]\n",
    "output = model(images,targets)   # Returns losses and detections\n",
    "# For inference\n",
    "model.eval()\n",
    "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
    "predictions = model(x)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2502a1e4356d273083c818ffae066d79aa408d0a6b3a1fb8ebf74f3257cfc1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
