{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3746bb02-7307-49c5-a408-879d74849f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class AnimalData:\n",
    "    def __init__(self,root,transforms):\n",
    "        self.root=root\n",
    "        self.transforms=transforms\n",
    "        classes=sort(list(os.listdir(root)))\n",
    "        images=[]\n",
    "        for c in classes:\n",
    "            if c=='Label':\n",
    "                continue\n",
    "            for img in sort(list(os.listdir(c))):\n",
    "                images.append(root+c+img)\n",
    "        self.images=images\n",
    "        label_dir=[c+'/Label' for c in classes]\n",
    "        labels=[]\n",
    "        for c in classes:\n",
    "            for label in sort(list(os.listdir(label_dir))):\n",
    "                labels.append(root+label_dir+label)\n",
    "        self.labels=labels\n",
    "    def __getitem__(self,idx):\n",
    "        img =Image.open(self.img[idx]).convert(\"RGB\")\n",
    "        with open(self.labels,'r') as f:\n",
    "            content= f.read()\n",
    "            lines=contents.split(' ')\n",
    "            for i in range(1, len(lines)):\n",
    "                lines[i]=int(lines[i])\n",
    "        \n",
    "        \"\"\"\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#from bs4 import BeautifulSoup\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "import tqdm \n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import random\n",
    "import os\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217cf022-932d-4b3d-955c-f29172ec9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels0=[]\n",
    "test_labels0=[]\n",
    "train_imgs0=[]\n",
    "test_imgs0=[]\n",
    "classes=['Bear', 'Bull', 'Cat', 'Cattle',\n",
    "           'Cheetah', 'Chicken', 'Crocodile',\n",
    "           'Deer', 'Dog', 'Fox', 'Goat', 'Hippopotamus', \n",
    "           'Horse', 'Jaguar', 'Leopard', 'Lion', 'Lynx',\n",
    "           'Monkey', 'Mule', 'Ostrich', 'Owl', \n",
    "           'Pig', 'Raccoon', 'Raven', 'Rhinoceros', \n",
    "           'Sheep', 'Snake', 'Tiger', 'Turkey', 'Turtle', 'Zebra']\n",
    "\"\"\"classes=['Cat','Dog']\"\"\"\n",
    "encode={}\n",
    "for i in range(len(classes)):\n",
    "    encode[classes[i]]=i+1\n",
    "\n",
    "def checkclass(dirname):\n",
    "    \n",
    "    for animal in classes:\n",
    "        if dirname.endswith(animal):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for dirname, dirs, filenames in os.walk('./data/'):\n",
    "    \n",
    "    if 'train' in dirname and checkclass(dirname):\n",
    "        labels= dirname +'/Label/'\n",
    "        \n",
    "        for label in sorted(os.listdir(labels)):\n",
    "            \n",
    "            \n",
    "            if label.endswith('txt'):\n",
    "                train_labels0.append(labels+label)\n",
    "        \n",
    "        for image in sorted(filenames):\n",
    "            \n",
    "            \n",
    "            if image.endswith('png') or image.endswith('jpg'):\n",
    "                train_imgs0.append(dirname+'/'+image)\n",
    "    if 'val' in dirname and checkclass(dirname):\n",
    "        labels= dirname +'/Label/'\n",
    "        for label in sorted(os.listdir(labels)):\n",
    "            if label.endswith('.txt'):\n",
    "                test_labels0.append(labels+label)\n",
    "        for image in sorted(filenames):\n",
    "            if image.endswith('png')or image.endswith('jpg'):\n",
    "                test_imgs0.append(dirname+'/'+image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9806e096-f681-445d-ba87-9197c87ac4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14620\n",
      "14620\n",
      "6864\n",
      "6864\n",
      "2924\n",
      "2924\n",
      "6864\n",
      "6864\n"
     ]
    }
   ],
   "source": [
    "print(len(train_imgs0))\n",
    "print(len(train_labels0))\n",
    "print(len(test_imgs0))\n",
    "print(len(test_labels0))\n",
    "train_labels=[]\n",
    "test_labels=[]\n",
    "train_imgs=[]\n",
    "test_imgs=[]\n",
    "for i in range(len(train_labels0)):\n",
    "    if i%5==0:\n",
    "        \n",
    "        train_labels.append(train_labels0[i])\n",
    "        train_imgs.append(train_imgs0[i])\n",
    "for i in range(len(test_labels0)):\n",
    "        test_labels.append(test_labels0[i])\n",
    "        test_imgs.append(test_imgs0[i])\n",
    "print(len(train_imgs))\n",
    "print(len(train_labels))\n",
    "print(len(test_imgs))\n",
    "print(len(test_labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d3c431-843f-4748-9e75-b0aa2001eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(object):\n",
    "    def __init__(self, transforms,img_dir,label_dir):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = img_dir\n",
    "        self.label=label_dir\n",
    "#         self.labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        file_image = self.imgs[idx]\n",
    "        file_label = self.label[idx]\n",
    "        img = Image.open(file_image).convert(\"RGB\")\n",
    "        target={}\n",
    "        target['image_id']=torch.tensor([idx])\n",
    "        labels=[]\n",
    "        boxes=[]\n",
    "        #Generate Label\n",
    "        with open(file_label,'r')as f:\n",
    "            lines=f.readlines()\n",
    "            for line in lines:\n",
    "                line_list=line.strip().split(' ')\n",
    "                labels.append(encode[line_list[0]])\n",
    "                boxes.append([float(i) for i in line_list[1:]])\n",
    "                \n",
    "            \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)        \n",
    "        \n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        \n",
    "        iscrowd = torch.zeros((len(classes),), dtype=torch.int64)\n",
    "\n",
    "        target['boxes']=boxes\n",
    "        target['labels']=labels\n",
    "       \n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f33b8b64-c012-4106-9715-36cdf9cd8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "import transforms as T\n",
    "\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    # converts the image, a PIL image, into a PyTorch Tensor\n",
    "    transforms.append(T.ToTensor())\n",
    "    if train:\n",
    "        # during training, randomly flip the training images\n",
    "        # and ground-truth for data augmentation\n",
    "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1d2250e-98d2-409a-a81a-87fd431349ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "\n",
    "def get_model_instance_mobilenet(num_classes):\n",
    "    \n",
    "    # load a model pre-trained on COCO\n",
    "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "    # replace the classifier with a new one, that has\n",
    "    # num_classes which is user-defined\n",
    "    num_classes = len(classes)+1  # 1 class (person) + background\n",
    "    # get number of input features for the classifier\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace the pre-trained head with a new one\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee1d38f-5761-450e-88a2-7a1dceeb5764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0]  [  0/731]  eta: 0:42:55  lr: 0.000012  loss: 4.3723 (4.3723)  loss_classifier: 3.4679 (3.4679)  loss_box_reg: 0.8497 (0.8497)  loss_objectness: 0.0067 (0.0067)  loss_rpn_box_reg: 0.0480 (0.0480)  time: 3.5233  data: 0.3310  max mem: 1598\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 43\u001b[0m\n\u001b[1;32m     39\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m     \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;66;03m# update the learning rate\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     torch\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./model/fasterRCNN/fasterRCNN_mobilenet.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/LQH/dl/Deeplearning/engine.py:46\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mexit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 46\u001b[0m \u001b[43mlosses\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lr_scheduler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "train_data=AnimalDataset(get_transform(train=False),train_imgs,train_labels)\n",
    "val_data=AnimalDataset(get_transform(train=False),train_imgs,train_labels)\n",
    "test_data=AnimalDataset(get_transform(train=False),test_imgs,test_labels)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "\n",
    "\n",
    "# define training and validation data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=4, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "\n",
    "model =get_model_instance_mobilenet(len(classes)+1)\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=7,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 50\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    torch.save(model.state_dict(), \"./model/fasterRCNN/fasterRCNN_mobilenet.pt\")\n",
    "    \n",
    "    print('saved dict')\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model,val_loader, device=device)\n",
    "    evaluate(model,test_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e8d4f2-50f2-4f72-974f-6fa60ba79d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode=[]\n",
    "for c in classes:\n",
    "    decode.append("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7a664821-1588-44c0-8667-9af4210f0f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cat']\n",
      "[tensor([ 66.2551, 233.8878, 456.1722, 608.0673], device='cuda:0')]\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from PIL import Image, ImageDraw,ImageFont\n",
    "# Load the model\n",
    "i=2499\n",
    "model =get_model_instance_mobilenet(len(classes)+1)\n",
    "model.load_state_dict(torch.load('./model/fasterRCNN/fasterRCNN_mobilenet.pt'))\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "test_data=AnimalDataset(get_transform(train=False),train_imgs[i:i+1],train_labels[i:i+1])\n",
    "# Load a batch of images\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "timecount=0\n",
    "for imgs,target in test_loader:\n",
    "    images = list(img.to('cuda') for img in imgs)\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    # Run the model on the images\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        outputs = model(images)\n",
    "    timecount+=time.time()-start_time\n",
    "    # Measure the end time\n",
    "    max_pred=torch.argmax(outputs[0]['scores'])\n",
    "    boxes = [outputs[0]['boxes'][max_pred]   ]  \n",
    "    # Load the image using PIL\n",
    "    \n",
    "\n",
    "    # Draw the boxes on the image\n",
    "    \n",
    "        \n",
    "\n",
    "    # Display the image\n",
    "    img = Image.open(train_imgs[i])\n",
    "\n",
    "    # Create a drawing object\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    # Define the font to use for the labels\n",
    "    font = ImageFont.truetype(\"/usr/share/fonts/truetype/abyssinica/AbyssinicaSIL-Regular.ttf\", 16)\n",
    "\n",
    "    # Define the bounding boxes and labels\n",
    "    \n",
    "    labels = [classes[outputs[0]['labels'][max_pred]-1]]\n",
    "    print(labels)\n",
    "    print(boxes)\n",
    "    # Draw the bounding boxes and labels\n",
    "    for box, label in zip(boxes, labels):\n",
    "        draw.rectangle(box.tolist(), outline=\"red\", width=2)\n",
    "        draw.text((box), label, font=font)\n",
    "\n",
    "    # Display the image\n",
    "    img.show()\n",
    "# Measure the start time\n",
    "\n",
    "\n",
    "# Calculate the inference time per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f649a4c-a6e4-422c-a4dd-ae6ad72ca475",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d9a60-6e8d-4049-b8b2-848b7f1f2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adaa0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=get_model_instance_mobilenet(len(classes)+1)\n",
    "model.to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4d4febd-c285-4fa0-84cb-56761b2cf5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=AnimalDataset(get_transform(train=False),test_imgs,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad1c0574-c8bc-45c8-854d-45f06a8f746c",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "targets should not be none when in training mode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Pass the preprocessed image to the model to get the output prediction\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 16\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Apply non-maximum suppression to remove redundant bounding boxes\u001b[39;00m\n\u001b[1;32m     19\u001b[0m pred \u001b[38;5;241m=\u001b[39m pred[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mxyxy[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torchvision/models/detection/generalized_rcnn.py:62\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[0;34m(self, images, targets)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m targets \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtargets should not be none when in training mode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m targets:\n",
      "File \u001b[0;32m~/anaconda3/envs/deeplearning/lib/python3.10/site-packages/torch/__init__.py:853\u001b[0m, in \u001b[0;36m_assert\u001b[0;34m(condition, message)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(condition) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;129;01mand\u001b[39;00m has_torch_function((condition,)):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(_assert, (condition,), condition, message)\n\u001b[0;32m--> 853\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m condition, message\n",
      "\u001b[0;31mAssertionError\u001b[0m: targets should not be none when in training mode"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import cv2\n",
    "\n",
    "# Load the trained YOLOv3 model\n",
    "\n",
    "\n",
    "# Load the image\n",
    "img = cv2.imread('data/val/Cat/00b6be538644be0b.jpg')\n",
    "\n",
    "# Preprocess the input image\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # Convert to RGB color space\n",
    "img = torch.from_numpy(img.transpose(2, 0, 1)).float().div(255.0).unsqueeze(0) # Normalize and convert to tensor\n",
    "\n",
    "# Pass the preprocessed image to the model to get the output prediction\n",
    "with torch.no_grad():\n",
    "    pred = model(img)\n",
    "\n",
    "# Apply non-maximum suppression to remove redundant bounding boxes\n",
    "pred = pred[0].xyxy[0]\n",
    "\n",
    "# Visualize the output prediction by drawing bounding boxes around the detected objects\n",
    "img = cv2.cvtColor(img[0].numpy().transpose(1, 2, 0), cv2.COLOR_RGB2BGR) # Convert back to BGR color space\n",
    "for box in pred:\n",
    "    cv2.rectangle(img, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow('Prediction', img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d97fc7f-be12-4f40-a6ad-3aa609446541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2502a1e4356d273083c818ffae066d79aa408d0a6b3a1fb8ebf74f3257cfc1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
