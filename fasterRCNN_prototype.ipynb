{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4281320-f5a7-4e79-983b-b5c8c5e089d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xmltodict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39metree\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mElementTree\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mET\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxmltodict\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xmltodict'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import xmltodict\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers as T\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e7ff9e54-d58a-4abc-a7d6-8fadfac6eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xml_handling(root='./archive', target_root='data'):\n",
    "    root='./archive'\n",
    "    target_root='data'\n",
    "    root_label=root+'/annotations'\n",
    "    root_img=root+'/images/'\n",
    "    \n",
    "    xml_files=os.listdir(root_label)    \n",
    "    i=0\n",
    "    for file in xml_files:\n",
    "        if file[0]=='.':\n",
    "            continue\n",
    "        file=root_label+\"/\"+file\n",
    "        train=False\n",
    "        if random.random()<0.8:\n",
    "            train=True\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        base_name=os.path.basename(file)\n",
    "        \n",
    "        base_name,ext=os.path.splitext(base_name)\n",
    "        result = ET.tostring(root,encoding='utf-8').decode('utf-8')\n",
    "        dict_data=xmltodict.parse(result)\n",
    "        data=dict_data['annotation']['object']\n",
    "        if len(data)==2:\n",
    "            continue\n",
    "        src_img=root_img+base_name+'.png'\n",
    "        img_path,label_path=\"\",\"\"\n",
    "        if data['name']=='cat':\n",
    "            note='Cat'+\" \"+str(data['bndbox']['xmin'])+\" \"+str(data['bndbox']['ymin'])+\" \"+str(data['bndbox']['xmax'])+\" \"+str(data['bndbox']['ymax'])\n",
    "            if train:\n",
    "                label_path=target_root+'/train/Cat/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/train/Cat/'+base_name+'.png'\n",
    "            else:\n",
    "                label_path=target_root+'/val/Cat/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/val/Cat/'+base_name+'.png'\n",
    "        else:\n",
    "            note='Dog'+\" \"+str(data['bndbox']['xmin'])+\" \"+str(data['bndbox']['ymin'])+\" \"+str(data['bndbox']['xmax'])+\" \"+str(data['bndbox']['ymax'])\n",
    "            \n",
    "            if train:\n",
    "                label_path=target_root+'/train/Dog/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/train/Dog/'+base_name+'.png'\n",
    "            else:\n",
    "                \n",
    "                label_path=target_root+'/val/Dog/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/val/Dog/'+base_name+'.png'\n",
    "        shutil.copy2(src_img,img_path)\n",
    "        print(label_path)\n",
    "        with open(label_path,'w') as f:\n",
    "            f.write(note)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3746bb02-7307-49c5-a408-879d74849f75",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"class AnimalData:\n",
    "    def __init__(self,root,transforms):\n",
    "        self.root=root\n",
    "        self.transforms=transforms\n",
    "        classes=sort(list(os.listdir(root)))\n",
    "        images=[]\n",
    "        for c in classes:\n",
    "            if c=='Label':\n",
    "                continue\n",
    "            for img in sort(list(os.listdir(c))):\n",
    "                images.append(root+c+img)\n",
    "        self.images=images\n",
    "        label_dir=[c+'/Label' for c in classes]\n",
    "        labels=[]\n",
    "        for c in classes:\n",
    "            for label in sort(list(os.listdir(label_dir))):\n",
    "                labels.append(root+label_dir+label)\n",
    "        self.labels=labels\n",
    "    def __getitem__(self,idx):\n",
    "        img =Image.open(self.img[idx]).convert(\"RGB\")\n",
    "        with open(self.labels,'r') as f:\n",
    "            content= f.read()\n",
    "            lines=contents.split(' ')\n",
    "            for i in range(1, len(lines)):\n",
    "                lines[i]=int(lines[i])\n",
    "        \n",
    "        \"\"\"\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#from bs4 import BeautifulSoup\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "import tqdm \n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import random\n",
    "import os\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "217cf022-932d-4b3d-955c-f29172ec9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=[]\n",
    "test_labels=[]\n",
    "train_imgs=[]\n",
    "test_imgs=[]\n",
    "classes=['Bear', 'Bull', 'Cat', 'Cattle',\n",
    "           'Cheetah', 'Chicken', 'Crocodile',\n",
    "           'Deer', 'Dog', 'Fox', 'Goat', 'Hippopotamus', \n",
    "           'Horse', 'Jaguar', 'Leopard', 'Lion', 'Lynx',\n",
    "           'Monkey', 'Mule', 'Ostrich', 'Owl', \n",
    "           'Pig', 'Raccoon', 'Raven', 'Rhinoceros',\n",
    "           'Sheep', 'Snake', 'Tiger', 'Turkey', 'Turtle', 'Zebra']\n",
    "\n",
    "\"\"\"\n",
    "classes=['Cat','Dog']\"\"\"\n",
    "num_classes=len(classes)\n",
    "encode={}\n",
    "for i in range(len(classes)):\n",
    "    encode[classes[i]]=i+1\n",
    "\n",
    "def checkclass(dirname):\n",
    "    \n",
    "    for animal in classes:\n",
    "        if dirname.endswith(animal):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for dirname, dirs, filenames in os.walk('./data/'):\n",
    "    \n",
    "    if 'train' in dirname and checkclass(dirname):\n",
    "        labels= dirname +'/Label/'\n",
    "      \n",
    "        for label in sorted(os.listdir(labels)):\n",
    "           \n",
    "            if  label.endswith('txt'):\n",
    "                train_labels.append(labels+label)\n",
    "      \n",
    "        for image in sorted(filenames):\n",
    "            \n",
    "            if image.endswith('png') or image.endswith('jpg'):\n",
    "                train_imgs.append(dirname+'/'+image)\n",
    "    if 'val' in dirname and checkclass(dirname):\n",
    "        labels= dirname +'/Label/'\n",
    "        for label in sorted(os.listdir(labels)):\n",
    "            if label.endswith('.txt'):\n",
    "                test_labels.append(labels+label)\n",
    "        for image in sorted(filenames):\n",
    "            if image.endswith('png') or image.endswith('jpg'):\n",
    "                test_imgs.append(dirname+'/'+image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9806e096-f681-445d-ba87-9197c87ac4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14620\n",
      "14620\n",
      "6864\n",
      "6864\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels))\n",
    "print(len(train_imgs))\n",
    "print(len(test_imgs))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14d3c431-843f-4748-9e75-b0aa2001eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(object):\n",
    "    def __init__(self, transforms,img_dir,label_dir):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = img_dir\n",
    "        self.label=label_dir\n",
    "#         self.labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        file_image = self.imgs[idx]\n",
    "        file_label = self.label[idx]\n",
    "        img = Image.open(file_image).convert(\"RGB\")\n",
    "        target={}\n",
    "        target['image_id']=torch.tensor([idx])\n",
    "        labels=[]\n",
    "        boxes=[]\n",
    "        #Generate Label\n",
    "        with open(file_label,'r')as f:\n",
    "            lines=f.readlines()\n",
    "            for line in lines:\n",
    "                line_list=line.strip().split(' ')\n",
    "                labels.append(encode[line_list[0]])\n",
    "                boxes.append([float(i) for i in line_list[1:]])\n",
    "                \n",
    "            if self.transforms is not None:\n",
    "                img=self.transforms(img)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)        \n",
    "        \n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        \n",
    "        iscrowd = torch.zeros((len(classes),), dtype=torch.int64)\n",
    "\n",
    "        target['boxes']=boxes\n",
    "        target['labels']=labels\n",
    "       \n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e3fe233c-0c69-4006-9c81-73c68b4aefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AugmentImageBboxes(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, image, bboxes):\n",
    "        # decide if flip or rotate\n",
    "        if random.random() < 0.5:\n",
    "            # Flip\n",
    "            image = transforms.functional.hflip(image)\n",
    "            h,w=image.shape[-2:]\n",
    "            bboxes[:, [0, 2]] = h - bboxes[:, [2, 0]] \n",
    "        else:\n",
    "            # Rotate by 90 degrees\n",
    "            image = transforms.functional.rotate(image, 90)\n",
    "            h, w = image.shape[-2:]\n",
    "            bboxes[:, [0, 1, 2, 3]] = [w - bboxes[:, 3], bboxes[:, 0], h - bboxes[:, 1], w - bboxes[:, 0]]\n",
    "        return image, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f33b8b64-c012-4106-9715-36cdf9cd8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3a689c3-71c8-474f-bf25-1cdd4ec93417",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d1d2250e-98d2-409a-a81a-87fd431349ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes =len(classes)+1 # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1ee1d38f-5761-450e-88a2-7a1dceeb5764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 8.00 GiB total capacity; 6.77 GiB already allocated; 0 bytes free; 6.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 40\u001b[0m\n\u001b[0;32m     36\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     38\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     39\u001b[0m     \u001b[39m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[1;32m---> 40\u001b[0m     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     41\u001b[0m     \u001b[39m# update the learning rate\u001b[39;00m\n\u001b[0;32m     42\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Pc\\deeplearning\\engine.py:30\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[0;32m     27\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     28\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m---> 30\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[0;32m     32\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m     34\u001b[0m \u001b[39m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     94\u001b[0m             degen_bb: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m boxes[bb_idx]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     95\u001b[0m             torch\u001b[39m.\u001b[39m_assert(\n\u001b[0;32m     96\u001b[0m                 \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAll bounding boxes should have positive height and width.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Found invalid box \u001b[39m\u001b[39m{\u001b[39;00mdegen_bb\u001b[39m}\u001b[39;00m\u001b[39m for target at index \u001b[39m\u001b[39m{\u001b[39;00mtarget_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m             )\n\u001b[1;32m--> 101\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensors)\n\u001b[0;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:57\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[1;32m---> 57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbody(x)\n\u001b[0;32m     58\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfpn(x)\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m out \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m name, module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 69\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers:\n\u001b[0;32m     71\u001b[0m         out_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\conv.py:459\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    455\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m    456\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[0;32m    457\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[0;32m    458\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[1;32m--> 459\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[0;32m    460\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 8.00 GiB total capacity; 6.77 GiB already allocated; 0 bytes free; 6.99 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "test_data=AnimalDataset(data_transform,test_imgs[:10],test_labels[:10])\n",
    "# our dataset has two classes only - background and person\n",
    "\n",
    "\n",
    "\n",
    "train_data= AnimalDataset(data_transform,train_imgs[:100],train_labels[:100])\n",
    "# define training and validation data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_data, batch_size=16, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model,test_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7a664821-1588-44c0-8667-9af4210f0f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 4964, 17424, 25808, 32616) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmpty\u001b[0m                                     Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1120\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1119\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1120\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_queue\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m   1121\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mTrue\u001b[39;00m, data)\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\multiprocessing\\queues.py:114\u001b[0m, in \u001b[0;36mQueue.get\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll(timeout):\n\u001b[1;32m--> 114\u001b[0m         \u001b[39mraise\u001b[39;00m Empty\n\u001b[0;32m    115\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_poll():\n",
      "\u001b[1;31mEmpty\u001b[0m: ",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m test_data\u001b[39m=\u001b[39mAnimalDataset(data_transform,test_imgs[:\u001b[39m10\u001b[39m],test_labels[:\u001b[39m10\u001b[39m])\n\u001b[0;32m      3\u001b[0m test_loader \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mDataLoader(\n\u001b[0;32m      4\u001b[0m     test_data, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, num_workers\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m,\n\u001b[0;32m      5\u001b[0m     collate_fn\u001b[39m=\u001b[39mutils\u001b[39m.\u001b[39mcollate_fn)\n\u001b[1;32m----> 6\u001b[0m evaluate(model,test_loader, device\u001b[39m=\u001b[39;49mdevice)\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Pc\\deeplearning\\engine.py:84\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, data_loader, device)\u001b[0m\n\u001b[0;32m     81\u001b[0m iou_types \u001b[39m=\u001b[39m _get_iou_types(model)\n\u001b[0;32m     82\u001b[0m coco_evaluator \u001b[39m=\u001b[39m CocoEvaluator(coco, iou_types)\n\u001b[1;32m---> 84\u001b[0m \u001b[39mfor\u001b[39;00m images, targets \u001b[39min\u001b[39;00m metric_logger\u001b[39m.\u001b[39mlog_every(data_loader, \u001b[39m100\u001b[39m, header):\n\u001b[0;32m     85\u001b[0m     images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(img\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m img \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     87\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39msynchronize()\n",
      "File \u001b[1;32mc:\\Users\\Pc\\deeplearning\\utils.py:209\u001b[0m, in \u001b[0;36mMetricLogger.log_every\u001b[1;34m(self, iterable, print_freq, header)\u001b[0m\n\u001b[0;32m    200\u001b[0m     log_msg \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdelimiter\u001b[39m.\u001b[39mjoin([\n\u001b[0;32m    201\u001b[0m         header,\n\u001b[0;32m    202\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m[\u001b[39m\u001b[39m{\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m \u001b[39m+\u001b[39m space_fmt \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m}/\u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mdata: \u001b[39m\u001b[39m{data}\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m    207\u001b[0m     ])\n\u001b[0;32m    208\u001b[0m MB \u001b[39m=\u001b[39m \u001b[39m1024.0\u001b[39m \u001b[39m*\u001b[39m \u001b[39m1024.0\u001b[39m\n\u001b[1;32m--> 209\u001b[0m \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m    210\u001b[0m     data_time\u001b[39m.\u001b[39mupdate(time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m end)\n\u001b[0;32m    211\u001b[0m     \u001b[39myield\u001b[39;00m obj\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1316\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_data(data)\n\u001b[0;32m   1315\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shutdown \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m-> 1316\u001b[0m idx, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_data()\n\u001b[0;32m   1317\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_tasks_outstanding \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable:\n\u001b[0;32m   1319\u001b[0m     \u001b[39m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1282\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[39m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[0;32m   1279\u001b[0m     \u001b[39m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[0;32m   1280\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1281\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m-> 1282\u001b[0m         success, data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_try_get_data()\n\u001b[0;32m   1283\u001b[0m         \u001b[39mif\u001b[39;00m success:\n\u001b[0;32m   1284\u001b[0m             \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:1133\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(failed_workers) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m   1132\u001b[0m     pids_str \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39mstr\u001b[39m(w\u001b[39m.\u001b[39mpid) \u001b[39mfor\u001b[39;00m w \u001b[39min\u001b[39;00m failed_workers)\n\u001b[1;32m-> 1133\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mDataLoader worker (pid(s) \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m) exited unexpectedly\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(pids_str)) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[0;32m   1134\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(e, queue\u001b[39m.\u001b[39mEmpty):\n\u001b[0;32m   1135\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39mFalse\u001b[39;00m, \u001b[39mNone\u001b[39;00m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 4964, 17424, 25808, 32616) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data=AnimalDataset(data_transform,test_imgs[:10],test_labels[:10])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "evaluate(model,test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f649a4c-a6e4-422c-a4dd-ae6ad72ca475",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d9a60-6e8d-4049-b8b2-848b7f1f2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdmodel="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2502a1e4356d273083c818ffae066d79aa408d0a6b3a1fb8ebf74f3257cfc1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
