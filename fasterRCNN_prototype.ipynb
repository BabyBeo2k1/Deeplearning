{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4281320-f5a7-4e79-983b-b5c8c5e089d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xmltodict'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39metree\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mElementTree\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mET\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mxmltodict\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xmltodict'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os\n",
    "import xmltodict\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import transformers as T\n",
    "import shutil\n",
    "import random\n",
    "from PIL import Image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e7ff9e54-d58a-4abc-a7d6-8fadfac6eee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def xml_handling(root='./archive', target_root='data'):\n",
    "    root='./archive'\n",
    "    target_root='data'\n",
    "    root_label=root+'/annotations'\n",
    "    root_img=root+'/images/'\n",
    "    \n",
    "    xml_files=os.listdir(root_label)    \n",
    "    i=0\n",
    "    for file in xml_files:\n",
    "        if file[0]=='.':\n",
    "            continue\n",
    "        file=root_label+\"/\"+file\n",
    "        train=False\n",
    "        if random.random()<0.8:\n",
    "            train=True\n",
    "        tree = ET.parse(file)\n",
    "        root = tree.getroot()\n",
    "        base_name=os.path.basename(file)\n",
    "        \n",
    "        base_name,ext=os.path.splitext(base_name)\n",
    "        result = ET.tostring(root,encoding='utf-8').decode('utf-8')\n",
    "        dict_data=xmltodict.parse(result)\n",
    "        data=dict_data['annotation']['object']\n",
    "        if len(data)==2:\n",
    "            continue\n",
    "        src_img=root_img+base_name+'.png'\n",
    "        img_path,label_path=\"\",\"\"\n",
    "        if data['name']=='cat':\n",
    "            note='Cat'+\" \"+str(data['bndbox']['xmin'])+\" \"+str(data['bndbox']['ymin'])+\" \"+str(data['bndbox']['xmax'])+\" \"+str(data['bndbox']['ymax'])\n",
    "            if train:\n",
    "                label_path=target_root+'/train/Cat/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/train/Cat/'+base_name+'.png'\n",
    "            else:\n",
    "                label_path=target_root+'/val/Cat/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/val/Cat/'+base_name+'.png'\n",
    "        else:\n",
    "            note='Dog'+\" \"+str(data['bndbox']['xmin'])+\" \"+str(data['bndbox']['ymin'])+\" \"+str(data['bndbox']['xmax'])+\" \"+str(data['bndbox']['ymax'])\n",
    "            \n",
    "            if train:\n",
    "                label_path=target_root+'/train/Dog/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/train/Dog/'+base_name+'.png'\n",
    "            else:\n",
    "                \n",
    "                label_path=target_root+'/val/Dog/Label/'+base_name+'.txt'\n",
    "                img_path=target_root+'/val/Dog/'+base_name+'.png'\n",
    "        shutil.copy2(src_img,img_path)\n",
    "        print(label_path)\n",
    "        with open(label_path,'w') as f:\n",
    "            f.write(note)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3746bb02-7307-49c5-a408-879d74849f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"class AnimalData:\n",
    "    def __init__(self,root,transforms):\n",
    "        self.root=root\n",
    "        self.transforms=transforms\n",
    "        classes=sort(list(os.listdir(root)))\n",
    "        images=[]\n",
    "        for c in classes:\n",
    "            if c=='Label':\n",
    "                continue\n",
    "            for img in sort(list(os.listdir(c))):\n",
    "                images.append(root+c+img)\n",
    "        self.images=images\n",
    "        label_dir=[c+'/Label' for c in classes]\n",
    "        labels=[]\n",
    "        for c in classes:\n",
    "            for label in sort(list(os.listdir(label_dir))):\n",
    "                labels.append(root+label_dir+label)\n",
    "        self.labels=labels\n",
    "    def __getitem__(self,idx):\n",
    "        img =Image.open(self.img[idx]).convert(\"RGB\")\n",
    "        with open(self.labels,'r') as f:\n",
    "            content= f.read()\n",
    "            lines=contents.split(' ')\n",
    "            for i in range(1, len(lines)):\n",
    "                lines[i]=int(lines[i])\n",
    "        \n",
    "        \"\"\"\n",
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "#from bs4 import BeautifulSoup\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets, models\n",
    "import torch\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
    "import matplotlib.patches as patches\n",
    "import tqdm \n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "import random\n",
    "import os\n",
    "\n",
    "# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "217cf022-932d-4b3d-955c-f29172ec9853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels=[]\n",
    "test_labels=[]\n",
    "train_imgs=[]\n",
    "test_imgs=[]\n",
    "classes=['Bear', 'Bull', 'Cat', 'Cattle',\n",
    "           'Cheetah', 'Chicken', 'Crocodile',\n",
    "           'Deer', 'Dog', 'Fox', 'Goat', 'Hippopotamus', \n",
    "           'Horse', 'Jaguar', 'Leopard', 'Lion', 'Lynx',\n",
    "           'Monkey', 'Mule', 'Ostrich', 'Owl', \n",
    "           'Pig', 'Raccoon', 'Raven', 'Rhinoceros',\n",
    "           'Sheep', 'Snake', 'Tiger', 'Turkey', 'Turtle', 'Zebra']\n",
    "\n",
    "\"\"\"\n",
    "classes=['Cat','Dog']\"\"\"\n",
    "num_classes=len(classes)\n",
    "encode={}\n",
    "for i in range(len(classes)):\n",
    "    encode[classes[i]]=i+1\n",
    "\n",
    "def checkclass(dirname):\n",
    "    \n",
    "    for animal in classes:\n",
    "        if dirname.endswith(animal):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "for dirname, dirs, filenames in os.walk('./data/'):\n",
    "    \n",
    "    if 'train' in dirname and checkclass(dirname):\n",
    "        labels= dirname +'/Label/'\n",
    "      \n",
    "        for label in sorted(os.listdir(labels)):\n",
    "           \n",
    "            if  label.endswith('txt'):\n",
    "                train_labels.append(labels+label)\n",
    "      \n",
    "        for image in sorted(filenames):\n",
    "            \n",
    "            if image.endswith('png') or image.endswith('jpg'):\n",
    "                train_imgs.append(dirname+'/'+image)\n",
    "    if 'val' in dirname and checkclass(dirname):\n",
    "        labels= dirname +'/Label/'\n",
    "        for label in sorted(os.listdir(labels)):\n",
    "            if label.endswith('.txt'):\n",
    "                test_labels.append(labels+label)\n",
    "        for image in sorted(filenames):\n",
    "            if image.endswith('png') or image.endswith('jpg'):\n",
    "                test_imgs.append(dirname+'/'+image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9806e096-f681-445d-ba87-9197c87ac4de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14620\n",
      "14620\n",
      "6864\n",
      "6864\n"
     ]
    }
   ],
   "source": [
    "print(len(train_labels))\n",
    "print(len(train_imgs))\n",
    "print(len(test_imgs))\n",
    "print(len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14d3c431-843f-4748-9e75-b0aa2001eeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(object):\n",
    "    def __init__(self, transforms,img_dir,label_dir):\n",
    "        self.transforms = transforms\n",
    "        # load all image files, sorting them to\n",
    "        # ensure that they are aligned\n",
    "        self.imgs = img_dir\n",
    "        self.label=label_dir\n",
    "#         self.labels = list(sorted(os.listdir(\"/kaggle/input/face-mask-detection/annotations/\")))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # load images ad masks\n",
    "        file_image = self.imgs[idx]\n",
    "        file_label = self.label[idx]\n",
    "        img = Image.open(file_image).convert(\"RGB\")\n",
    "        target={}\n",
    "        target['image_id']=torch.tensor([idx])\n",
    "        labels=[]\n",
    "        boxes=[]\n",
    "        #Generate Label\n",
    "        with open(file_label,'r')as f:\n",
    "            lines=f.readlines()\n",
    "            for line in lines:\n",
    "                line_list=line.strip().split(' ')\n",
    "                labels.append(encode[line_list[0]])\n",
    "                boxes.append([float(i) for i in line_list[1:]])\n",
    "                \n",
    "            if self.transforms is not None:\n",
    "                img=self.transforms(img)\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        # Labels (In my case, I only one class: target class or background)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)        \n",
    "        \n",
    "        \n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        \n",
    "        \n",
    "        iscrowd = torch.zeros((len(classes),), dtype=torch.int64)\n",
    "\n",
    "        target['boxes']=boxes\n",
    "        target['labels']=labels\n",
    "       \n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        return img, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3fe233c-0c69-4006-9c81-73c68b4aefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AugmentImageBboxes(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __call__(self, image, bboxes):\n",
    "        # decide if flip or rotate\n",
    "        if random.random() < 0.5:\n",
    "            # Flip\n",
    "            image = transforms.functional.hflip(image)\n",
    "            h,w=image.shape[-2:]\n",
    "            bboxes[:, [0, 2]] = h - bboxes[:, [2, 0]] \n",
    "        else:\n",
    "            # Rotate by 90 degrees\n",
    "            image = transforms.functional.rotate(image, 90)\n",
    "            h, w = image.shape[-2:]\n",
    "            bboxes[:, [0, 1, 2, 3]] = [w - bboxes[:, 3], bboxes[:, 0], h - bboxes[:, 1], w - bboxes[:, 0]]\n",
    "        return image, bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f33b8b64-c012-4106-9715-36cdf9cd8297",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3a689c3-71c8-474f-bf25-1cdd4ec93417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[0.5647, 0.5686, 0.5725,  ..., 0.3059, 0.3098, 0.3098],\n",
       "          [0.5686, 0.5725, 0.5765,  ..., 0.2980, 0.3020, 0.3098],\n",
       "          [0.5608, 0.5686, 0.5765,  ..., 0.2941, 0.3059, 0.3059],\n",
       "          ...,\n",
       "          [0.8824, 0.9255, 0.9608,  ..., 0.9843, 0.9843, 0.9608],\n",
       "          [0.9176, 0.9176, 0.9333,  ..., 0.9843, 0.9725, 0.9725],\n",
       "          [0.9490, 0.9373, 0.9412,  ..., 0.9804, 0.9725, 0.9765]],\n",
       " \n",
       "         [[0.5216, 0.5255, 0.5255,  ..., 0.2039, 0.2078, 0.2078],\n",
       "          [0.5216, 0.5255, 0.5294,  ..., 0.2078, 0.2118, 0.2078],\n",
       "          [0.5137, 0.5216, 0.5294,  ..., 0.2078, 0.2157, 0.2157],\n",
       "          ...,\n",
       "          [0.7647, 0.8078, 0.8471,  ..., 0.8627, 0.8627, 0.8392],\n",
       "          [0.7843, 0.7843, 0.8000,  ..., 0.8549, 0.8471, 0.8471],\n",
       "          [0.8157, 0.8039, 0.8078,  ..., 0.8510, 0.8471, 0.8510]],\n",
       " \n",
       "         [[0.4353, 0.4392, 0.4392,  ..., 0.1529, 0.1569, 0.1569],\n",
       "          [0.4353, 0.4392, 0.4431,  ..., 0.1529, 0.1569, 0.1569],\n",
       "          [0.4275, 0.4353, 0.4353,  ..., 0.1529, 0.1608, 0.1529],\n",
       "          ...,\n",
       "          [0.6157, 0.6588, 0.6902,  ..., 0.6902, 0.6902, 0.6667],\n",
       "          [0.6392, 0.6392, 0.6510,  ..., 0.6941, 0.6863, 0.6941],\n",
       "          [0.6706, 0.6588, 0.6588,  ..., 0.6902, 0.6863, 0.6980]]]),\n",
       " {'image_id': tensor([0]),\n",
       "  'boxes': tensor([[192.7997,  81.5522, 848.0000, 605.2450]]),\n",
       "  'labels': tensor([1]),\n",
       "  'area': tensor([343123.6562]),\n",
       "  'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0])})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "dataset = AnimalDataset(data_transform,train_imgs[:10000],train_labels[:10000])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1d2250e-98d2-409a-a81a-87fd431349ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# load a model pre-trained on COCO\n",
    "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
    "\n",
    "# replace the classifier with a new one, that has\n",
    "# num_classes which is user-defined\n",
    "num_classes =len(classes)+1 # 1 class (person) + background\n",
    "# get number of input features for the classifier\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# replace the pre-trained head with a new one\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4978b8a3-63c3-49c5-aa83-e3e6fae137d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'image_id': tensor([0], device='cuda:0'), 'boxes': tensor([[192.7997,  81.5522, 848.0000, 605.2450]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([343123.6562], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([1], device='cuda:0'), 'boxes': tensor([[389.1200, 148.0047, 800.6400, 649.6826]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([206450.4688], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([2], device='cuda:0'), 'boxes': tensor([[  12.8000,    0.0000, 1022.5776,  482.5766]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([487295.0312], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([3], device='cuda:0'), 'boxes': tensor([[533.0002, 237.0002, 810.9998, 430.9998]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([53931.8359], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([4], device='cuda:0'), 'boxes': tensor([[ 321.4223,   14.2187, 1014.7553,  691.0312]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([469256.5000], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([5], device='cuda:0'), 'boxes': tensor([[127.2230, 266.9855, 610.8035, 579.2461]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([151003.1250], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([6], device='cuda:0'), 'boxes': tensor([[ 55.7424,   0.0000, 682.3594, 923.5200]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([578693.3750], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([7], device='cuda:0'), 'boxes': tensor([[ 40.9600, 171.6007, 864.0000, 678.7195]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([417379.0625], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([8], device='cuda:0'), 'boxes': tensor([[330.0004, 123.9997, 938.0004, 745.9999]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([378176.0625], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([9], device='cuda:0'), 'boxes': tensor([[334.3995, 180.8003, 722.4003, 759.2003]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([224419.6719], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([10], device='cuda:0'), 'boxes': tensor([[314.2400,  56.3398, 641.2800, 618.4587]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([183835.4062], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([11], device='cuda:0'), 'boxes': tensor([[ 77.5015, 211.8400, 589.9103, 951.6800]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([379100.4688], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([12], device='cuda:0'), 'boxes': tensor([[ 26.8800,  73.0414, 530.5600, 521.5402]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([225899.8594], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([13], device='cuda:0'), 'boxes': tensor([[102.4000, 119.1293, 873.6000, 418.2329]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([230668.6875], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([14], device='cuda:0'), 'boxes': tensor([[419.8400, 497.9197, 576.0000, 595.2000]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([15191.2881], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}, {'image_id': tensor([15], device='cuda:0'), 'boxes': tensor([[356.4800, 208.7382, 566.4000, 605.0851]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'area': tensor([83201.1406], device='cuda:0'), 'iscrowd': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0], device='cuda:0')}]\n",
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    " dataset, batch_size=16, collate_fn=collate_fn)\n",
    "for imgs, annotations in data_loader:\n",
    "    imgs = list(img.to(device) for img in imgs)\n",
    "    annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "    print(annotations)\n",
    "    break\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1ee1d38f-5761-450e-88a2-7a1dceeb5764",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 8.00 GiB total capacity; 6.19 GiB already allocated; 0 bytes free; 6.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m num_epochs \u001b[39m=\u001b[39m \u001b[39m10\u001b[39m\n\u001b[0;32m     36\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_epochs):\n\u001b[0;32m     37\u001b[0m     \u001b[39m# train for one epoch, printing every 10 iterations\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n\u001b[0;32m     39\u001b[0m     \u001b[39m# update the learning rate\u001b[39;00m\n\u001b[0;32m     40\u001b[0m     lr_scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\Pc\\deeplearning\\engine.py:30\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[1;34m(model, optimizer, data_loader, device, epoch, print_freq)\u001b[0m\n\u001b[0;32m     27\u001b[0m images \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(image\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m images)\n\u001b[0;32m     28\u001b[0m targets \u001b[39m=\u001b[39m [{k: v\u001b[39m.\u001b[39mto(device) \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m t\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m targets]\n\u001b[1;32m---> 30\u001b[0m loss_dict \u001b[39m=\u001b[39m model(images, targets)\n\u001b[0;32m     32\u001b[0m losses \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(loss \u001b[39mfor\u001b[39;00m loss \u001b[39min\u001b[39;00m loss_dict\u001b[39m.\u001b[39mvalues())\n\u001b[0;32m     34\u001b[0m \u001b[39m# reduce losses over all GPUs for logging purposes\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:101\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     94\u001b[0m             degen_bb: List[\u001b[39mfloat\u001b[39m] \u001b[39m=\u001b[39m boxes[bb_idx]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m     95\u001b[0m             torch\u001b[39m.\u001b[39m_assert(\n\u001b[0;32m     96\u001b[0m                 \u001b[39mFalse\u001b[39;00m,\n\u001b[0;32m     97\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mAll bounding boxes should have positive height and width.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m Found invalid box \u001b[39m\u001b[39m{\u001b[39;00mdegen_bb\u001b[39m}\u001b[39;00m\u001b[39m for target at index \u001b[39m\u001b[39m{\u001b[39;00mtarget_idx\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     99\u001b[0m             )\n\u001b[1;32m--> 101\u001b[0m features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images\u001b[39m.\u001b[39;49mtensors)\n\u001b[0;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(features, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m    103\u001b[0m     features \u001b[39m=\u001b[39m OrderedDict([(\u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m, features)])\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\detection\\backbone_utils.py:57\u001b[0m, in \u001b[0;36mBackboneWithFPN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, Tensor]:\n\u001b[1;32m---> 57\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbody(x)\n\u001b[0;32m     58\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfpn(x)\n\u001b[0;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\models\\_utils.py:69\u001b[0m, in \u001b[0;36mIntermediateLayerGetter.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     67\u001b[0m out \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m     68\u001b[0m \u001b[39mfor\u001b[39;00m name, module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems():\n\u001b[1;32m---> 69\u001b[0m     x \u001b[39m=\u001b[39m module(x)\n\u001b[0;32m     70\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers:\n\u001b[0;32m     71\u001b[0m         out_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_layers[name]\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torch\\nn\\modules\\module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Pc\\anaconda3\\envs\\deeplearning\\lib\\site-packages\\torchvision\\ops\\misc.py:62\u001b[0m, in \u001b[0;36mFrozenBatchNorm2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     60\u001b[0m scale \u001b[39m=\u001b[39m w \u001b[39m*\u001b[39m (rv \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meps)\u001b[39m.\u001b[39mrsqrt()\n\u001b[0;32m     61\u001b[0m bias \u001b[39m=\u001b[39m b \u001b[39m-\u001b[39m rm \u001b[39m*\u001b[39m scale\n\u001b[1;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m x \u001b[39m*\u001b[39;49m scale \u001b[39m+\u001b[39m bias\n",
      "\u001b[1;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 8.00 GiB total capacity; 6.19 GiB already allocated; 0 bytes free; 6.97 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from engine import train_one_epoch, evaluate\n",
    "import utils\n",
    "test_data=AnimalDataset(data_transform,test_imgs[:10],test_labels[:10])\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "# our dataset has two classes only - background and person\n",
    "\n",
    "\n",
    "# define training and validation data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset, batch_size=16, shuffle=True, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "\n",
    "# get the model using our helper function\n",
    "\n",
    "\n",
    "# move model to the right device\n",
    "model.to(device)\n",
    "\n",
    "# construct an optimizer\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005,\n",
    "                            momentum=0.9, weight_decay=0.0005)\n",
    "# and a learning rate scheduler\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                step_size=3,\n",
    "                                                gamma=0.1)\n",
    "\n",
    "# let's train it for 10 epochs\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # train for one epoch, printing every 10 iterations\n",
    "    train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=10)\n",
    "    # update the learning rate\n",
    "    lr_scheduler.step()\n",
    "    # evaluate on the test dataset\n",
    "    evaluate(model,test_loader, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7a664821-1588-44c0-8667-9af4210f0f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_data=AnimalDataset(data_transform,test_imgs[:10],test_labels[:10])\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data, batch_size=1, shuffle=False, num_workers=4,\n",
    "    collate_fn=utils.collate_fn)\n",
    "evaluate(model,test_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f649a4c-a6e4-422c-a4dd-ae6ad72ca475",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d9a60-6e8d-4049-b8b2-848b7f1f2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ssdmodel="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "2502a1e4356d273083c818ffae066d79aa408d0a6b3a1fb8ebf74f3257cfc1ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
